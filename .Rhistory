# Use chi-square for categorical variables and t-tests for numeric vs. income
# Example: Chi-square test for association between marital_status and income
chisq.test(table(train_data$income, train_data$sex))
chisq.test(table(train_data$marital_status, train_data$income))
# Plot distributions
boxplot(age ~ income, data = train_data, main = "Age vs Income", col="purple")
# Load the dataset without headers
adult_data <- read.csv("/Users/apple/desktop/ML Assignment 3/adult.data", header = FALSE, na.strings = " ?")
# Assign column names manually
colnames(adult_data) <- c("age", "workclass", "fnlwgt", "education", "education_num",
"marital_status", "occupation", "relationship", "race",
"sex", "capital_gain", "capital_loss", "hours_per_week",
"native_country", "income")
# Inspect the dataset
str(adult_data)
summary(adult_data)
View(adult_data)
# Identify data types and categorical/ordinal variables
# Load the dataset without headers
adult_data <- read.csv("/Users/apple/desktop/ML Assignment 3/adult.data", header = FALSE, na.strings = " ?")
# Assign column names manually
colnames(adult_data) <- c("age", "workclass", "fnlwgt", "education", "education_num",
"marital_status", "occupation", "relationship", "race",
"sex", "capital_gain", "capital_loss", "hours_per_week",
"native_country", "income")
# Inspect the dataset
str(adult_data)
summary(adult_data)
# Check missing values
sapply(adult_data, function(x) sum(is.na(x)))
# Check missing values
# Check and replace " ?" with NA using `na.strings` parameter (already done during data loading)
# Confirm replacement
sum(is.na(adult_data))
# Check missing values
# Check and replace " ?" with NA using `na.strings` parameter (already done during data loading)
# Confirm replacement
adult_data <- read.csv("/Users/apple/desktop/ML Assignment 3/adult.data", header = FALSE, na.strings = " ?")
sum(is.na(adult_data))
# Check missing values
sapply(adult_data, function(x) sum(is.na(x)))
# Load the Dataset
adult_data <- read.csv("/Users/apple/desktop/ML Assignment 3/adult.data", header = FALSE, na.strings = " ?")
colnames(adult_data) <- c("age", "workclass", "fnlwgt", "education", "education_num",
"marital_status", "occupation", "relationship", "race",
"sex", "capital_gain", "capital_loss", "hours_per_week",
"native_country", "income")
# Inspecting data structure
str(adult_data)
summary(adult_data)
# Load the Dataset
adult_data <- read.csv("/Users/apple/desktop/ML Assignment 3/adult.data", header = FALSE, na.strings = " ?")
colnames(adult_data) <- c("age", "workclass", "fnlwgt", "education", "education_num",
"marital_status", "occupation", "relationship", "race",
"sex", "capital_gain", "capital_loss", "hours_per_week",
"native_country", "income")
# Inspecting data structure
str(adult_data)
View(adult_data)
summary(adult_data)
adult_data[adult_data == " ?"] <- NA
summary(adult_data)
adult_data[adult_data == " ?"] <- NA
summary(adult_data)
# Check missing values
sapply(adult_data, function(x) sum(is.na(x)))
# Check missing values
sum(is.na(adult_data))
sapply(adult_data, function(x) sum(is.na(x)))
sapply(adult_data, function(adult_data) sum(is.na(adult_data)))
sapply(adult_data, function(adult_data) sum(is.na(adult_data)))
set.seed(42)
library(caret)
trainIndex <- createDataPartition(adult_data$income, p = 0.8, list = FALSE)
train_data <- adult_data[trainIndex, ]
test_data <- adult_data[-trainIndex, ]
library(caret)
set.seed(42)
library(caret)
trainIndex <- createDataPartition(adult_data$income, p = 0.8, list = FALSE)
train_data <- adult_data[trainIndex, ]
test_data <- adult_data[-trainIndex, ]
for (col in names(train_data)) {
if (is.factor(train_data[[col]])) {
mode_value <- names(sort(table(train_data[[col]]), decreasing = TRUE))[1]
train_data[[col]][is.na(train_data[[col]])] <- mode_value
} else {
mean_value <- mean(train_data[[col]], na.rm = TRUE)
train_data[[col]][is.na(train_data[[col]])] <- mean_value
}
}
for (col in names(train_data)) {
if (is.factor(train_data[[col]])) {
mode_value <- names(sort(table(train_data[[col]]), decreasing = TRUE))[1]
train_data[[col]][is.na(train_data[[col]])] <- mode_value
} else {
mean_value <- mean(train_data[[col]], na.rm = TRUE)
train_data[[col]][is.na(train_data[[col]])] <- mean_value
}
}
# Check missing values in train and test datasets
sapply(train_data, function(x) sum(is.na(x)))
sapply(test_data, function(x) sum(is.na(x)))
# Implement chosen imputation methods, e.g., mode imputation for categorical variables
# Check missing values in train and test datasets
sapply(train_data, function(adult_data) sum(is.na(adult_data)))
sapply(test_data, function(adult_data) sum(is.na(adult_data)))
# Implement chosen imputation methods, e.g., mode imputation for categorical variables
## Example of combining infrequent levels into an "Other" category based on frequency
threshold <- 0.01 * nrow(train_data)
frequent_countries <- names(which(table(train_data$native_country) >= threshold))
train_data$native_country <- ifelse(train_data$native_country %in% frequent_countries,
train_data$native_country, "Other")
## Example of combining infrequent levels into an "Other" category based on frequency
library(dplyr)
native_country_freq <- table(train_data$native_country)
train_data$native_country <- ifelse(native_country_freq[train_data$native_country] < 0.001 * nrow(train_data), "Other", train_data$native_country)
test_data$native_country <- ifelse(native_country_freq[test_data$native_country] < 0.001 * nrow(test_data), "Other", test_data$native_country)
threshold <- 0.01 * nrow(train_data)
frequent_countries <- names(which(table(train_data$native_country) >= threshold))
train_data$native_country <- ifelse(train_data$native_country %in% frequent_countries,
train_data$native_country, "Other")
install.packages("ggplot2")
library(ggplot2)
library(ggplot2)
ggplot(train_data, aes(x = workclass, fill = income)) + geom_bar(position = "fill")
train_data$income <- as.factor(ifelse(train_data$income == " >50K", 1, 0))
test_data$income <- as.factor(ifelse(test_data$income == " >50K", 1, 0))
logistic_model <- glm(income ~ ., data = train_data, family = "binomial")
# Predict on test set
pred_probs <- predict(logistic_model, test_data, type = "response")
pred_labels <- as.factor(ifelse(pred_probs > 0.5, 1, 0))
set.seed(42)
library(caret)
trainIndex <- createDataPartition(adult_data$income, p = 0.8, list = FALSE)
train_data <- adult_data[trainIndex, ]
test_data <- adult_data[-trainIndex, ]
set.seed(42)
library(caret)
trainIndex <- createDataPartition(adult_data$income, p = 0.8, list = FALSE)
train_data <- adult_data[trainIndex, ]
test_data <- adult_data[-trainIndex, ]
set.seed(42)
library(caret)
trainIndex <- createDataPartition(adult_data$income, p = 0.8, list = FALSE)
train_data <- adult_data[trainIndex, ]
test_data <- adult_data[-trainIndex, ]
library(dplyr)
# Impute numeric columns with median and categorical columns with mode
train_data <- train_data %>%
mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
mutate(across(where(is.factor), ~ fct_explicit_na(.)))
test_data <- test_data %>%
mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
mutate(across(where(is.factor), ~ fct_explicit_na(.)))
library(dplyr)
# Impute numeric columns with median and categorical columns with mode
train_data <- train_data %>%
mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
mutate(across(where(is.factor), ~ fct_explicit_na(.)))
test_data <- test_data %>%
mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
mutate(across(where(is.factor), ~ fct_explicit_na(.)))
library(dplyr)
# Impute numeric columns with median and categorical columns with mode
train_data <- train_data %>%
mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
mutate(across(where(is.factor), ~ fct_explicit_na(.)))
test_data <- test_data %>%
mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
mutate(across(where(is.factor), ~ fct_explicit_na(.)))
library(dplyr)
# Impute numeric columns with median and categorical columns with mode
train_data <- train_data %>%
mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
mutate(across(where(is.factor), ~ fct_explicit_na(.)))
test_data <- test_data %>%
mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
mutate(across(where(is.factor), ~ fct_explicit_na(.)))
# Check missing values in train and test datasets
sapply(train_data, function(adult_data) sum(is.na(adult_data)))
sapply(test_data, function(adult_data) sum(is.na(adult_data)))
# Use chi-square for categorical variables and t-tests for numeric vs. income
chisq.test(table(train_data$income, train_data$sex))
chisq.test(table(train_data$marital_status, train_data$income))
# Plot distributions
boxplot(age ~ income, data = train_data, main = "Age vs Income", col="purple")
# Logistic regression model
model_logit <- glm(income ~ ., data = train_data, family = binomial)
library(C50)
# Train C5.0 model on the original training data
C50_model <- C5.0(income ~ ., data = train_data, trials = 30)
R.version.string
install.packages("reticulate")
install.packages("tensorflow")
install.packages("keras")
library(tensorflow)
library(tensorflow)
nstall_tensorflow(method = "conda", envname = "r-tensorflow-compatible")
library(tensorflow)
install_tensorflow(method = "conda", envname = "r-tensorflow-compatible")
library(tensorflow)
use_condaenv("r-tensorflow-compatible", required = TRUE)
library(keras3)  # YOU MIGHT WANT library(keras3)
tf$constant("Hello, TensorFlow!")
concrete <- read.csv("concrete.csv")
str(concrete)
concrete_train <- concrete[1:773,-9]
train_labels<-concrete[1:773,9]
concrete_test <- concrete[774:1030,-9]
test_labels <- concrete[774:1030,9]
concrete_train=scale(concrete_train)
col_means_train <- attr(concrete_train, "scaled:center")
col_stddevs_train <- attr(concrete_train, "scaled:scale")
#avoid data leakage
concrete_test <- scale(concrete_test, center = col_means_train, scale = col_stddevs_train)
library(keras3) # YOU MIGHT WANT library(keras3)
model <- keras_model_sequential() %>%
layer_dense(units = 10, activation = "relu", input_shape = c(dim(concrete_train)[2])) %>%
layer_dense(units = 1)
# Compile the model
model %>% compile(
optimizer = 'adam',
loss = 'mse'
)
model
set.seed(1)
history <- model %>% fit(concrete_train,
train_labels,
batch_size=50,
epochs = 500,
validation_data=list(concrete_test,test_labels))
predictions=model %>% predict(concrete_test)
rmse= function(x,y){
return((mean((x - y)^2))^0.5)
}
rmse(predictions,test_labels)
plot(history)
knitr::opts_chunk$set(echo = TRUE)
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(caret)
# Load the data
bikes <- read.csv("bike.csv")
# Explore data structure and summary statistics
str(bikes)
summary(bikes)
# Check for missing values
sum(is.na(bikes))
# Plot the histogram of the target variable "count"
ggplot(bikes, aes(x = count)) +
geom_histogram(binwidth = 50, fill = "purple", color = "black") +
labs(title = "Histogram of Bike Rental Counts", x = "Count", y = "Frequency")
View(bikes)
# Remove 'registered' and 'casual' columns
bikes <- bikes %>% select(-registered, -casual)
View(bikes)
# Apply square root transformation to 'count'
bikes$count <- sqrt(bikes$count)
# Plot histogram again after transformation
ggplot(bikes, aes(x = count)) +
geom_histogram(binwidth = 1, fill = "purple", color = "black") +
labs(title = "Histogram of Square Root Transformed Bike Rental Counts", x = "Square Root of Count", y = "Frequency")
# Convert 'datetime' to date-time format and extract components
bikes$datetime <- as.POSIXlt(bikes$datetime)
bikes$year <- as.numeric(format(bikes$datetime, "%Y"))
bikes$month <- as.numeric(format(bikes$datetime, "%m"))
bikes$day_of_month <- as.numeric(format(bikes$datetime, "%d"))
bikes$day_of_week <- as.numeric(format(bikes$datetime, "%u"))
bikes$hour <- as.numeric(format(bikes$datetime, "%H"))
# Remove original 'datetime' variable
bikes$datetime <- NULL
# Transform cyclic variables using sine and cosine transformations
bikes$month_x <- cos(2 * pi * bikes$month / 12)
bikes$month_y <- sin(2 * pi * bikes$month / 12)
bikes$day_of_week_x <- cos(2 * pi * bikes$day_of_week / 7)
bikes$day_of_week_y <- sin(2 * pi * bikes$day_of_week / 7)
bikes$hour_x <- cos(2 * pi * bikes$hour / 24)
bikes$hour_y <- sin(2 * pi * bikes$hour / 24)
bikes$season_x <- cos(2 * pi * bikes$season / 4)
bikes$season_y <- sin(2 * pi * bikes$season / 4)
# Remove the original cyclic variables
bikes <- bikes %>% select(-month, -day_of_week, -hour, -season)
# Convert categorical variables to factors for one-hot encoding
bikes$weather <- as.factor(bikes$weather)
# One-hot encode categorical variables
bikes <- model.matrix(~ . - 1, data = bikes) %>% as.data.frame()
View(bikes)
# Set seed for reproducibility
set.seed(2024)
# Split data into 90% train and 10% test
inTrain <- createDataPartition(bikes$count, p = 0.9, list = FALSE)
bikes_train <- bikes[inTrain, ]
bikes_test <- bikes[-inTrain, ]
# Further split training data into 90% training and 10% validation
inTrain2 <- createDataPartition(bikes_train$count, p = 0.9, list = FALSE)
bikes_train_final <- bikes_train[inTrain2, ]
bikes_validation <- bikes_train[-inTrain2, ]
# Scale numeric variables in training data
numeric_vars <- names(bikes_train_final)[sapply(bikes_train_final, is.numeric) & names(bikes_train_final) != "count"]
scaling_params <- preProcess(bikes_train_final[, numeric_vars], method = c("center", "scale"))
# Apply scaling to train, validation, and test sets
bikes_train_final[, numeric_vars] <- predict(scaling_params, bikes_train_final[, numeric_vars])
bikes_validation[, numeric_vars] <- predict(scaling_params, bikes_validation[, numeric_vars])
bikes_test[, numeric_vars] <- predict(scaling_params, bikes_test[, numeric_vars])
# Scaling function
scale_data <- function(train_data, validation_data, test_data, exclude_col) {
numeric_cols <- sapply(train_data, is.numeric)
numeric_cols[exclude_col] <- FALSE
# Compute means and sds on training data
means <- colMeans(train_data[, numeric_cols])
sds <- apply(train_data[, numeric_cols], 2, sd)
# Scale data
train_data[, numeric_cols] <- scale(train_data[, numeric_cols], center = means, scale = sds)
validation_data[, numeric_cols] <- scale(validation_data[, numeric_cols], center = means, scale = sds)
test_data[, numeric_cols] <- scale(test_data[, numeric_cols], center = means, scale = sds)
list(train = train_data, validation = validation_data, test = test_data)
}
# Assuming 'count' is the target column
scaled_data <- scale_data(bikes_train_final, bikes_validation, bikes_test, exclude_col = "count")
bikes_train_final <- scaled_data$train
bikes_validation <- scaled_data$validation
bikes_test <- scaled_data$test
library(keras3)
library(tfruns)
# Define hyperparameter flags
FLAGS <- flags(
flag_integer("nodes1", 64),
flag_integer("nodes2", 32),
flag_string("activation", "relu"),
flag_integer("batch_size", 32),
flag_numeric("learning_rate", 0.001)
)
# Define the model-building function
build_model <- function() {
model <- keras_model_sequential() %>%
layer_dense(units = FLAGS$nodes1, activation = FLAGS$activation, input_shape = ncol(bikes_train_final) - 1) %>%
layer_dense(units = FLAGS$nodes2, activation = FLAGS$activation) %>%
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_adam(lr = FLAGS$learning_rate),
loss = "mse"
)
return(model)
}
# Instantiate the model
model <- build_model()
reticulate::py_last_error()
reticulate::py_last_error()$r_trace$full_call
reticulate::py_run_string("print('Python is working')")
library(tfruns)
FLAGS <- flags(
flag_integer("hidden1_units", c(32, 64, 128)),  # Number of nodes for hidden layer 1
flag_integer("hidden2_units", c(32, 64, 128)),  # Number of nodes for hidden layer 2
flag_string("activation", c("relu", "sigmoid")), # Activation function
flag_integer("batch_size", c(32, 64, 128)),      # Batch size
flag_numeric("learning_rate", c(0.001, 0.01, 0.1))  # Learning rate
)
library(keras3)
library(tfruns)
FLAGS <- flags(
flag_integer("hidden1_units", c(32, 64, 128)),  # Number of nodes for hidden layer 1
flag_integer("hidden2_units", c(32, 64, 128)),  # Number of nodes for hidden layer 2
flag_string("activation", c("relu", "sigmoid")), # Activation function
flag_integer("batch_size", c(32, 64, 128)),      # Batch size
flag_numeric("learning_rate", c(0.001, 0.01, 0.1))  # Learning rate
)
library(keras3)
library(tfruns)
# Define hyperparameter flags
FLAGS <- flags(
flag_integer("nodes1", 64),
flag_integer("nodes2", 32),
flag_string("activation", "relu"),
flag_integer("batch_size", 32),
flag_numeric("learning_rate", 0.001)
)
# Define the model-building function
build_model <- function() {
model <- keras_model_sequential() %>%
layer_dense(units = FLAGS$nodes1, activation = FLAGS$activation, input_shape = ncol(bikes_train_final) - 1) %>%
layer_dense(units = FLAGS$nodes2, activation = FLAGS$activation) %>%
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_adam(lr = FLAGS$learning_rate),
loss = "mse"
)
return(model)
}
# Instantiate the model
model <- build_model()
library(keras3)
library(tfruns)
# Define hyperparameter flags
FLAGS <- flags(
flag_integer("nodes1", 64),
flag_integer("nodes2", 32),
flag_string("activation", "relu"),
flag_integer("batch_size", 32),
flag_numeric("learning_rate", 0.001)
)
# Define the model-building function
build_model <- function() {
model <- keras_model_sequential() %>%
layer_dense(units = FLAGS$nodes1, activation = FLAGS$activation, input_shape = ncol(bikes_train_final) - 1) %>%
layer_dense(units = FLAGS$nodes2, activation = FLAGS$activation) %>%
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_adam(lr = FLAGS$learning_rate),
loss = "mse"
)
return(model)
}
library(keras3)
library(tfruns)
# Define hyperparameter flags
FLAGS <- flags(
flag_integer("nodes1", 64),
flag_integer("nodes2", 32),
flag_string("activation", "relu"),
flag_integer("batch_size", 32),
flag_numeric("learning_rate", 0.001)
)
# Define the model-building function
build_model <- function() {
model <- keras_model_sequential() %>%
layer_dense(units = FLAGS$nodes1, activation = FLAGS$activation, input_shape = ncol(bikes_train_final) - 1) %>%
layer_dense(units = FLAGS$nodes2, activation = FLAGS$activation) %>%
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_adam(lr = FLAGS$learning_rate),
loss = "mse"
)
return(model)
}
# Instantiate the model
model <- build_model()
FLAGS <- flags(
flag_numeric("nodes_h1", 32),
flag_numeric("nodes_h2", 16),
flag_string("activation", "relu"),
flag_numeric("batch_size", 32),
flag_numeric("learning_rate", 0.001)
)
# Define the model architecture using keras
model <- keras_model_sequential() %>%
layer_dense(units = FLAGS$nodes_h1, activation = FLAGS$activation) %>%
layer_dense(units = FLAGS$nodes_h2, activation = FLAGS$activation) %>%
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_adam(lr = FLAGS$learning_rate),
loss = "mse"
)
FLAGS <- flags(
flag_numeric("nodes_h1", 32),
flag_numeric("nodes_h2", 16),
flag_string("activation", "relu"),
flag_numeric("batch_size", 32),
flag_numeric("learning_rate", 0.01)
)
# Define the model architecture using keras
model <- keras_model_sequential() %>%
layer_dense(units = FLAGS$nodes_h1, activation = FLAGS$activation) %>%
layer_dense(units = FLAGS$nodes_h2, activation = FLAGS$activation) %>%
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_adam(lr = FLAGS$learning_rate),
loss = "mse"
)
library(tensorflow)
use_condaenv("r-tensorflow-compatible", required = TRUE)
library(keras3)
tf$constant("Hello, TensorFlow!")
concrete <- read.csv("concrete.csv")
library(tensorflow)
use_condaenv("r-tensorflow-compatible", required = TRUE)
library(keras3)
tf$constant("Hello, TensorFlow!")
concrete <- read.csv("concrete.csv")
str(concrete)
concrete_train <- concrete[1:773,-9]
train_labels<-concrete[1:773,9]
concrete_test <- concrete[774:1030,-9]
test_labels <- concrete[774:1030,9]
concrete_train=scale(concrete_train)
col_means_train <- attr(concrete_train, "scaled:center")
col_stddevs_train <- attr(concrete_train, "scaled:scale")
#avoid data leakage
concrete_test <- scale(concrete_test, center = col_means_train, scale = col_stddevs_train)
library(keras3)
model <- keras_model_sequential() %>%
layer_dense(units = 10, activation = "relu", input_shape = c(dim(concrete_train)[2])) %>%
layer_dense(units = 1)
# Compile the model
model %>% compile(
optimizer = 'adam',
loss = 'mse'
)
model
set.seed(1)
history <- model %>% fit(concrete_train,
train_labels,
batch_size=50,
epochs = 500,
validation_data=list(concrete_test,test_labels))
plot(history)
predictions=model %>% predict(concrete_test)
rmse= function(x,y){
return((mean((x - y)^2))^0.5)
}
rmse(predictions,test_labels)
